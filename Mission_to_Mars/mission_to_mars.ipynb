{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import pymongo\n",
    "from splinter import Browser\n",
    "from bs4 import BeautifulSoup\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Establish db connection\n",
    "conn = \"mongodb://localhost:27017\"\n",
    "client = pymongo.MongoClient(conn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the Mongo database\n",
    "# Declare the db\n",
    "db = client.mars_db\n",
    "\n",
    "# Declare the collection\n",
    "collection = db.mars"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Define executable path and initialize the browser\n",
    "executable_path = {'executable_path': ChromeDriverManager().install()}\n",
    "browser = Browser('chrome', **executable_path, headless=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NASA Mars News Scraping\n",
    "\n",
    "[Mars News Site](https://redplanetscience.com/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the browser\n",
    "url1 = 'https://redplanetscience.com/'\n",
    "browser.visit(url1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Parse into Beautiful Soup object\n",
    "news_html = browser.html\n",
    "soup = BeautifulSoup(news_html, \"html.parser\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Iterate through all cards of the news in the bootstrap code\n",
    "# Retrieve all elements that contain News Titles and Paragraph Text\n",
    "articles = soup.find_all(\"div\", class_ = \"list_text\")\n",
    "   \n",
    "# Iterate through each card and retrieve headline and paragraph\n",
    "for article in articles:\n",
    "       \n",
    "    # Use BeautifulSoup's find() method to navigate and retrieve attributes\n",
    "    news_title = article.find(\"div\", class_ = \"content_title\").text\n",
    "    news_body = article.find(\"div\", class_ = \"article_teaser_body\").text\n",
    "    \n",
    "    print(\"------------------------------------------\")\n",
    "    print(f\"Headline:  {news_title}\")\n",
    "    print(f\"Content:  {news_body}\")\n",
    "   \n",
    "    # Dictionary to be inserted as a MongoDB document\n",
    "    post = {\"Headline\": news_title, \n",
    "            \"Content\": news_body,\n",
    "           }\n",
    "           \n",
    "    collection.insert_one(post)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify database results \n",
    "results = collection.find()\n",
    "for result in results:\n",
    "    print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Close remote browser\n",
    "# browser.quit()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## JPL Mars Space Images Scraping\n",
    "\n",
    "[Featured Space Image site](https://spaceimages-mars.com)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Define executable path and initialize the browser\n",
    "executable_path = {'executable_path': ChromeDriverManager().install()}\n",
    "browser = Browser('chrome', **executable_path, headless=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the browser\n",
    "url2 = 'https://spaceimages-mars.com/'\n",
    "browser.visit(url2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Parse into Beautiful Soup object\n",
    "image_html = browser.html\n",
    "soup = BeautifulSoup(image_html, \"html.parser\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Find the image url\n",
    "image_path = soup.find(\"img\", class_ = \"headerimage\")[\"src\"]\n",
    "print(image_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "featured_img_url = \"https://spaceimages-mars.com/\"+image_path\n",
    "print(featured_img_url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Close remote browser\n",
    "# browser.quit()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mars Facts\n",
    "\n",
    "[Mars Facts site](https://galaxyfacts-mars.com)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Scrape the table using pandas\n",
    "url3 = \"https://galaxyfacts-mars.com/\"\n",
    "    \n",
    "tables = pd.read_html(url3)\n",
    "\n",
    "tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First Table \n",
    "df1 = tables[0]\n",
    "df1.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop the Earth column\n",
    "df1 = df1.drop([2], axis=1)\n",
    "df1.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rename headers\n",
    "df1 = df1.rename(columns = {0:'Fact', 1:'Data'})\n",
    "df1.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop the first row\n",
    "df1[df1.Fact != \"Mars - Earth Comparison\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Second Table \n",
    "df2 = tables[1]\n",
    "df2.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rename headers\n",
    "df2 = df2.rename(columns = {0:'Fact', 1:'Data'})\n",
    "df2.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "facts_df = df1.append(df2, ignore_index=True, sort=False)\n",
    "\n",
    "facts_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parse to an html string\n",
    "fact_table = facts_df.to_html()\n",
    "fact_table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Close remote browser\n",
    "# browser.quit()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mars Hemispheres\n",
    "\n",
    "[Mars Hemispheres site](https://marshemispheres.com/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the browser\n",
    "url4 = 'https://marshemispheres.com/'\n",
    "browser.visit(url4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parse into Beautiful Soup object\n",
    "hemi_html = browser.html\n",
    "soup = BeautifulSoup(hemi_html, \"html.parser\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save image url string and hemisphere title to a Python dictionary\n",
    "hemisphere_img_urls = [{\"title\":\"Cerberus Hemisphere\",\"img_url\": \"https://marshemispheres.com/images/cerberus_enhanced.tif\"},\n",
    "                       {\"title\":\"Schiaparelli Hemisphere\",\"img_url\": \"https://marshemispheres.com/images/schiaparelli_enhanced.tif\"},\n",
    "                       {\"title\":\"Syrtis Major Hemisphere\",\"img_url\": \"https://marshemispheres.com/images/syrtis_major_enhanced.tif\"},\n",
    "                       {\"title\":\"Valles Marineris Hemisphere\",\"img_url\": \"https://marshemispheres.com/images/valles_marineris_enhanced.tif\"},\n",
    "                      ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hemisphere_img_urls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Can this be automated in a loop?\n",
    "# Initialize the browser\n",
    "url5 = \"https://marshemispheres.com/\"\n",
    "browser.visit(url5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parse into Beautiful Soup object\n",
    "hemi2_html = browser.html\n",
    "soup = BeautifulSoup(hemi2_html, \"html.parser\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Retrieve all elements that contain image URLs\n",
    "links = hemispheresSoup.find_all(\"div\", class_ = \"description\")\n",
    "\n",
    "# Create empty list hold dictionaries\n",
    "hemisphere_image_urls = []\n",
    "\n",
    "# Iterate through\n",
    "for link in links:\n",
    "        \n",
    "    # Use BeautifulSoup's find() method to navigate and retrieve attributes\n",
    "    img_title = link.find(\"h3\").text\n",
    "    img_link = link.find(\"a\", class_ = \"itemLink product-item\")[\"href\"]\n",
    "    \n",
    "    # Find the link with the full res image\n",
    "    browser.visit(url5 + img_link)\n",
    "    \n",
    "    # HTML object\n",
    "    link = browser.html\n",
    "    \n",
    "    # Parse HTML with BeautifulSoup\n",
    "    linksoup = BeautifulSoup(link, \"html.parser\")\n",
    "    \n",
    "    # Full resolution image URL\n",
    "    url = url5 + linksoup.find(\"img\", class_ = \"wide-image\")[\"src\"]\n",
    "    \n",
    "    # Append to list of dictionaries\n",
    "    hemisphere_image_urls.append({\"title\":img_title, \"img_url\":url})\n",
    "    \n",
    "    # Display titles and links\n",
    "    print(\"------------------------------------------\")\n",
    "    print(f\"Title:  {img_title}\")\n",
    "    print(f\"Link:   {url}\")\n",
    "\n",
    "# Display end statement    \n",
    "print(\" \")\n",
    "print(\"Scraping Complete.\")\n",
    "\n",
    "# Close remote browser\n",
    "browser.quit()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
